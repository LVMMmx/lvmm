{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![LVMM logo](https://lvmm.mx/wp-content/uploads/2019/05/LVMM_logo_500-90x90.png)\n",
        "\n",
        "# Manual de uso del clúster LVMM"
      ],
      "metadata": {
        "id": "JPm6Ci2B0JLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ssh\"></a>\n",
        "## 1. ¿Cómo acceder al cluster LVMM?\n",
        "\n",
        "La coneción al clúster computacional del [Departamento de Modelación de Nanomateriales](https://www.cnyn.unam.mx/?page_id=659) del [CNyN-UNAM](https://www.cnyn.unam.mx) es por medio del protocolo [OpenSSH](https://www.openssh.com/) (Open Secure Shell, por sus siglas en inglés)."
      ],
      "metadata": {
        "id": "12NLSVz6FB9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dentro de la red del CNyN ésto se realiza por medio del siguiente comando:\n",
        "\n",
        "```bash\n",
        "$ ssh usuario@192.168.100.237\n",
        "```\n",
        "\n",
        "donde `usuario` se refiere al nombre del usuario con el que se quiere acceder al clúster, una vez establecida la comunicación el servidor nos preguntará nuestra clave:\n",
        "\n",
        "```bash\n",
        "By accessing this system, you consent to the following conditions:\n",
        "- This system is for authorized use only.\n",
        "- Any or all uses of this system and all files on this system may be monitored.\n",
        "- Communications using, or data stored on, this system are not private.\n",
        "\n",
        "usuario@192.168.100.237's password:\n",
        "\n",
        "```\n",
        "\n",
        "Una vez logrando igresar al clúster LVMM somos bienvenidos con el símbolo del sistema:\n",
        "\n",
        "```bash\n",
        "usuario@lvmm:~$\n",
        "```\n"
      ],
      "metadata": {
        "id": "7Jw0Pz4sTWm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"sftp\"></a>\n",
        "## 2. ¿Cómo transferir información al clúster LVMM?\n",
        "\n",
        "Para transferir información entre el cluster se utiliza el mismo protocolo `SSH` utilizando las herrmientas `sftp` y/o `scp`."
      ],
      "metadata": {
        "id": "51MCQrLza-4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`sftp` se refiere a la implementación del servicio FTP (File Transfer Protocol, por sus siglas en inglés) sobre el protocolo SSH, la forma más básica para utilizarlo sería de la siguiente manera:\n",
        "\n",
        "```bash\n",
        "$ sftp usuario@192.168.100.237\n",
        "```\n",
        "\n",
        "otra forma de transferir archivos puede ser con la herramienta `scp`, una implementación de comando `cp` (copiar) sobre el protocolo SSH, la forma de usarlo es:\n",
        "\n",
        "> **scp** [opciones] \\<**origen**> \\<**destino**>\n",
        "\n",
        "en caso de que el \\<**origen**> ó \\<**destino**> sean remoto éstos tomarían la sigueinte forma:\n",
        "\n",
        "> usuario@192.168.100.237:\\<**ruta al archivo/directorio**>\n",
        "\n",
        "si es un directorio no olvide incluir la opción **-r** para que el copiado se realice de manera recursivo, por ejemplo para copiar archivos remotos a la PC **localhost** se invocaría de la siguiente manera:\n",
        "\n",
        "```bash\n",
        "localhost:~/resultados $ scp -r usuario@192.168.100.237:/tmpu/grupo/usuario/datos .\n",
        "```\n",
        "\n",
        "ésto realizaría el copiado del directorio remoto `/tmpu/grupo/usuario/datos` hospedado en el servidor `192.168.100.237` accediendo como el usuario `usuario` al directorio actual `.` (recordando que el directorio `.` se refiere al directorio en el que se encuentra) de la PC `localhost`."
      ],
      "metadata": {
        "id": "YehWBIyxSxYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"module\"></a>\n",
        "# 3. ¿Cómo utilizar los programas instalados en el clúster LVMM?\n",
        "\n",
        "Para poder utilizar los programas ya instalados en el cúster LVMM es necesario cargar el módulo adecuado, ([Environment Modules](https://modules.sourceforge.net/))."
      ],
      "metadata": {
        "id": "ApIATjmlGWGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ml_av\"></a>\n",
        "Para ver la lista de aplicaciones, librerías y utilidades, ésta se pueden consultar con el siguiente comando:\n",
        "\n",
        "```bash\n",
        "$ module available\n",
        "```\n",
        "\n",
        "o en su forma adbreviada:\n",
        "\n",
        "```bash\n",
        "$ ml av\n",
        "```\n",
        "\n",
        "```bash\n",
        "----------------------------------- /share/modules/Applications ------------------------------------\n",
        "abinit/10.0.7.1-intel    gromacs/2024.1-gnu-mkl     vasp/6.4.1-intel           \n",
        "amber/20-Update12        irrep/1.1                  vasp/6.4.1-intel-vtst         \n",
        "autodock/4.2.6-intel     lammps/2Aug2023-intel      vasp/6.4.1-MD-intel           \n",
        "boltztrap/20.7.1         namd/2.14-intel            vasp/6.4.1-nvhpc-gpu            \n",
        "cp2k/2024.1-gnu-mkl      py4vasp/0.5.1              wannier90/2.1.0-intel\n",
        "critic2/1.1.git-intel    qe/7.1-intel               wannier90/3.1.0-intel         \n",
        "crystal17/1.0.2-intel    siesta/honpas-4.1.5-intel  wannier90/3.1.0-serial-intel  \n",
        "dftb+/24.1-intel         tinker/8.10.2-intel        wien2k/23.2-intel             \n",
        "gromacs/2024.1-gnu-cuda  \n",
        "\n",
        "Key:\n",
        "default-version  modulepath  \n",
        "```\n"
      ],
      "metadata": {
        "id": "PTVNbIGJT4mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ml_load\"></a>\n",
        "Si queremos utilizar, por ejemplo, el programa VASP para GPU, debemos primero cargar el módulo `vasp/6.4.1-nvhpc-gpu` con el siguiente comando:\n",
        "\n",
        "```bash\n",
        "$ ml load vasp/6.4.1-nvhpc-gpu\n",
        "```\n",
        "\n",
        "\n",
        "```bash\n",
        "Loading vasp version 6.4.1\n",
        "Loading mkl version 2023.0.0\n",
        "Loading tbb version 2021.8.0\n",
        "Loading compiler-rt version 2023.0.0\n",
        "Loading fftw version 3.3.10\n",
        "Loading hdf5 version 1.14.3\n",
        "\n",
        "Loading vasp/6.4.1-nvhpc-gpu\n",
        "  Loading requirement: nvhpc/24.5 tbb/latest compiler-rt/latest mkl/latest fftw/3.3.10-nvhpc\n",
        "    hdf5/1.14.3-nvhpc\n",
        "```\n",
        "\n",
        "Los mensajes de salida indican que también se han cargado las dependencias requeridas para que funcione correctamente nuestra aplicación, una vez realizado ésto podémos correr `vasp_std`, `vasp_ncl` o `vasp_gam`.\n",
        "\n",
        "```bash\n",
        "$ mpirun -n 4 vasp_ncl\n",
        " running    4 mpi-ranks, with    2 threads/rank, on    1 nodes\n",
        " distrk:  each k-point on    4 cores,    1 groups\n",
        " distr:  one band on    1 cores,    4 groups\n",
        " vasp.6.4.1 05Apr23 (build Jul  1 2024 01:14:50) complex                         \n",
        " ```\n",
        "\n",
        "con éste comando se ejecuta el programa `vasp_ncl` en el **nodo maestro**, lo que **NO es recomendado**)."
      ],
      "metadata": {
        "id": "-XEpvH0tdH_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"slurm\"></a>\n",
        "# 4. ¿Cómo correr una aplicación en el LVMM?\n",
        "\n",
        "El clúster HPC LVMM cuenta con el sistema de administración de recursos [SLURM](https://slurm.schedmd.com) (Simple Linux Utility for Resource Management, por sus siglas en inglés)."
      ],
      "metadata": {
        "id": "FrAD26NQ6xBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"sinfo\"></a>\n",
        "## 4.1 sinfo\n",
        "\n",
        "El comando `sinfo` nos ayuda a obtener información sobre el cluster,\n",
        "\n",
        "```shell\n",
        "$ sinfo\n",
        "```\n",
        "\n",
        "```shell\n",
        "PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
        "q_gpu        up   infinite      2   idle compute-0-[0,5]\n",
        "q_mem        up   infinite      1    mix compute-0-1\n",
        "q_cpu*       up   infinite      3    mix compute-0-[1-2,4]\n",
        "q_cpu*       up   infinite      1  alloc compute-0-3\n",
        "```\n",
        "éste comando nos muestra las distintas **particiones** del clúster, podemos ver que ésta instalación de `SLURM` se cuenta con 3 particiones distintas: `q_gpu`, `q_mem` y `q_cpu`, también podemos ver el éstado de los nodos que conforman éstas particiones; La partición `q_gpu` se encuentra **desocupada** y que cuenta de los nodos de cómputo: `compute-0-0` y `compute-0-5`; también podemos ver que en la partición `q_cpu`, el nodo `compute-0-3` se encuentra **ocupado** y los nodos `compute-0-1`, `compute-0-2` y `compute-0-4` se encuentran en un éstado **mixto**.\n",
        "\n",
        "Podemos obtener la misma información pero ahora centrada en los nodos en lugar de las particiones con la opción `--Node` o en la forma adbreviada `-N`,\n",
        "\n",
        "```bash\n",
        "$ sinfo --Node\n",
        "NODELIST     NODES PARTITION STATE\n",
        "compute-0-0      1     q_gpu idle  \n",
        "compute-0-1      1    q_cpu* mix   \n",
        "compute-0-1      1     q_mem mix   \n",
        "compute-0-2      1    q_cpu* mix   \n",
        "compute-0-3      1    q_cpu* alloc\n",
        "compute-0-4      1    q_cpu* mix   \n",
        "compute-0-5      1     q_gpu idle  \n",
        "```\n"
      ],
      "metadata": {
        "id": "rlO_Hh-dUCzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"srun\"></a>\n",
        "## 4.2 srun\n",
        "\n",
        "Por ejemplo, para poder correr `VASP` en el cluster se puede ejecutar con la instrucción `srun` de la siguiente manera:\n",
        "\n",
        "```bash\n",
        "$ srun --partition q_mem --ntasks 16 vasp_ncl\n",
        "```\n",
        "```bash\n",
        " running   16 mpi-ranks, on    1 nodes\n",
        " distrk:  each k-point on   16 cores,    1 groups\n",
        " distr:  one band on    4 cores,    4 groups\n",
        " vasp.6.4.1 05Apr23 (build Feb 23 2024 02:44:29) complex                        \n",
        "```\n",
        "\n",
        "con ésto corremos el programa `vasp_ncl` en la partición `q_mem` con 16 tarea, el inconveniente de ésta forma de correr el programa es que tenemos que esperar hasta que estén disponibles los recursos requeridos, al igual que durante su ejecución.\n"
      ],
      "metadata": {
        "id": "cqd2cEl-j1Sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"sbatch\"></a>\n",
        "## 4.3 sbatch\n",
        "\n",
        "La forma idónea para mandar un trabajo a una cola de ejecución es por medio del comando `sbatch`, para ésto tenemos que gerenar un script:\n",
        "\n",
        "`vasp.job:`\n",
        "```shell\n",
        "#!/usr/bin/env bash\n",
        "srun vasp_ncl\n",
        "```\n",
        "una vez creado el script se somete el script con el comando `sbatch`de la sigueinte forma:\n",
        "\n",
        "```bash\n",
        "$ sbatch --ntasks=16 --nodes=1 --partition=q_cpu vasp.job\n",
        "```\n",
        "```\n",
        "Submitted batch job 176\n",
        "```\n",
        "ésto nos regresa un número `176`, el cual es el `ID` del trabajo.\n",
        "\n",
        "podemos agregar las opciones de la linea de comando de `sbatch` al mismo script, junto con otras opciones, de la siguiente forma:\n",
        "\n",
        "`vasp.job:`\n",
        "```shell\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=VASP_test        # Nombre para identificar el trabajo (-J)\n",
        "#SBATCH --nodes=1                   # utilizar sólo un nodo (-N)\n",
        "#SBATCH --ntasks=16                 # ejecitar 16 tareas (-n)\n",
        "#SBATCH --cpus-per-task=1           # CPUs por tarea (-c)\n",
        "#SBATCH --ntasks-per-node=16        # tareas por nodo\n",
        "#SBATCH --output vasp_test-%j.o     # el nombre del archivo a donde escribir las salidas de la ejecución (-o)\n",
        "#SBATCH --error  vasp_test-%j.e     # nombre del archivo para escribir los errores de ejecución (-e)\n",
        "# %j se refiere al ID asignado a la hora de someter el script\n",
        "#SBATCH --partition=q_cpu           # partición en la cual ejecutar el script -(p)\n",
        "\n",
        ". /etc/profile.d/modules.sh        # Inicializar el sistema de módulos\n",
        "ml vasp/6.4.1-intel                # cargar el módulo adecuado\n",
        "\n",
        "srun vasp_ncl                      # ejecutar vast_ncl en los recursos solicitados\n",
        "```\n",
        "para someter éste último script sólo tenemos que mandar como parametro el nombre del script (`vasp.job`) a `sbatch`:\n",
        "\n",
        "```bash\n",
        "$ sbatch vasp.job\n",
        "```\n",
        "```shell\n",
        "Submitted batch job 177\n",
        "```\n"
      ],
      "metadata": {
        "id": "jjWUWDVcj9SJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"squeue\"></a>\n",
        "## 4.4 squeue\n",
        "\n",
        "Para ver el éstado de los trabajos sometidos a `Slurm` podemos utilizar la instrucción `squeue`.\n",
        "\n",
        "```bash\n",
        "$ squeue\n",
        "```\n",
        "```bash\n",
        "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "             164       q_cpu  model_4   jperez  R 16-10:19:13     1 compute-0-3\n",
        "             174       q_cpu    model   jperez  R 4-07:57:35      1 compute-0-2\n",
        "             173       q_cpu     VASP vgalingo  R 2-12:45:11      1 compute-0-1\n",
        "             175       q_cpu     Test  wmedina  R   22:59:14      1 compute-0-2\n",
        "             171       q_cpu      aex  wmedina  R 1-20:57:02      1 compute-0-3\n",
        "             172       q_cpu   10-35s   clfdez  R    7:05:41      1 compute-0-1\n",
        "             177       q_cpu VASP_tes    suser  R      16:32      1 compute-0-1\n",
        "             178       q_cpu model_dm  emarmol  R    6:06:50      1 compute-0-3\n",
        "             182       q_cpu model_fm vgalindo  R    5:47:16      1 compute-0-4\n",
        "             184       q_cpu      igc  ncampos  R      56:32      1 compute-0-1\n",
        "             186       q_mem Na2SO2_d    jvzmg  R    7:05:41      1 compute-0-1\n",
        "```\n",
        "\n",
        "podemos verificar que nuestro trabajo esta corriendo (`R`), con la opción `--user $USER` podemos ver sólo nuestros trabajos:\n",
        "\n",
        "```bash\n",
        "$ squeue --user $USER\n",
        "```\n",
        "\n",
        "```bash\n",
        "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "             177       q_cpu VASP_tes    suser  R      18:22      1 compute-0-1\n",
        "```\n"
      ],
      "metadata": {
        "id": "B9Uje1r7kDuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"scancel\"></a>\n",
        "## 4.5 scancel\n",
        "\n",
        "Se puede cancelar o retirar un trabajo de la cola con el comando `scancel`:\n",
        "\n",
        "```bash\n",
        "$ scancel 177\n",
        "```\n",
        "\n",
        "```bash\n",
        "$ squeue -u $USER\n",
        "```\n",
        "```bash\n",
        "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "             177         cpu VASP_tes    suser  CA      18:22      1 compute-0-1\n",
        "```\n",
        "podemos ver que el trabajo cambió de estado a cancelando (CA). También podemos utilizar el nombre el trabajo `VASP_test` con la opción `--name`,\n",
        "\n",
        "```bash\n",
        "$ scancel --name VASP_test\n",
        "```\n"
      ],
      "metadata": {
        "id": "dmOVxnTakJCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"salloc\"></a>\n",
        "## 4.6 salloc\n",
        "\n",
        "También podemos ejecutar nuestros programas de forma interactiva utilizando el comando `salloc`, el cual nos aloja los recursos solicitados y nos da un shell ineractivo en que podemor realizar pruebas y diagnosticar errores,\n",
        "\n",
        "```bash\n",
        "$ salloc -p q_cpu -N 1 -n 16 -J VASP_test -t 60\n",
        "```\n",
        "```bash\n",
        "salloc: Granted job allocation 178\n",
        "$\n",
        "```\n",
        "una vez terminada nuestra ejecución o pruebas nos podemos salir como en cualquier shell con\n",
        "\n",
        "```bash\n",
        "$ exit\n",
        "```\n",
        "ó\n",
        "\n",
        "```bash\n",
        "$ logout\n",
        "```\n",
        "\n",
        "o también se puede utilizar el acceso rápido `ctrl + d`\n"
      ],
      "metadata": {
        "id": "BzRKshuekMTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 5. Referencias:\n",
        "\n",
        "- The SLURM website: https://slurm.schedmd.com/\n",
        "\n",
        "- The SLURM documentation: https://slurm.schedmd.com/documentation.html\n",
        "\n",
        "- The SLURM user community: https://groups.google.com/g/slurm-users"
      ],
      "metadata": {
        "id": "QX0OqurV6UzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aldo Rodríguez Guerrero, 2024"
      ],
      "metadata": {
        "id": "rF1IJkYp6kmY"
      }
    }
  ]
}